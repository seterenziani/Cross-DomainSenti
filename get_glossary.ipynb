{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import bigrams\n",
    "import scipy.sparse\n",
    "import json \n",
    "import os.path\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import chi2,SelectKBest\n",
    "from sklearn import pipeline\n",
    "from sklearn.metrics import f1_score,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path): \n",
    "    df = pd.read_json(path, lines = True)\n",
    "    return df \n",
    "\n",
    "def write_file(path, file, text): \n",
    "    pathname = os.path.join(path, file)\n",
    "    outfile = open(pathname, 'w')\n",
    "    for instance in text:\n",
    "        outfile.write(json.dumps(instance) + '\\n')\n",
    "    outfile.close()\n",
    "    \n",
    "def flatten_list(df, column): \n",
    "    lst = []\n",
    "    for i in df[column]:\n",
    "        flatten_lst = []\n",
    "        for j in i: \n",
    "            for item in j: \n",
    "                flatten_lst.append(item)\n",
    "        lst.append(flatten_lst)\n",
    "    df[column] = lst\n",
    "    return df \n",
    "\n",
    "def get_tuples(df, column): \n",
    "    lst = []\n",
    "    for i in df[column]:\n",
    "        list_tuples = []\n",
    "        for j in i: \n",
    "            list_tuples.append(tuple(j)) \n",
    "        lst.append(list_tuples)\n",
    "    df[column] = lst\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code inspiration from: https://github.com/isabellin105/word-embeddings/blob/master/HW_3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reviews(data, text):\n",
    "    listTokenisedReviews = []\n",
    "    for review in data[text]: \n",
    "        listTokenisedReviews.append(review)\n",
    "    numTokenisedReviews = len(listTokenisedReviews)\n",
    "    return listTokenisedReviews, numTokenisedReviews\n",
    "\n",
    "def get_vocabulary(tokenized, minimum, maximum): \n",
    "    #Function. Input: tokenized documents. Returns: a dictionary of vocabulary and word-index loopup dictionary \n",
    "    #Function. Takes only frequently appearing words into consideration. \n",
    "    #Parameters: (1) tokenized: a list of tokenised strings\n",
    "    #Parameters: (2)minimum: minimum unigram counts\n",
    "    #Parameters: (3)maximum: maximum unigram count\n",
    "    #Output: (1) vocab: a counter where vocab[word] is the count of unigrams's occurrence in all documents \n",
    "    #Output: (2) word2ind: a word to index loopup dictionary for words in vobaculary dictionary \n",
    "    #Output: (3) idx2word: a index to word loopup dictionary for words in vobaculary dictionary \n",
    "    vocab = Counter()\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence: \n",
    "            vocab[word] += 1\n",
    "    print('%d vocabs before' % len(vocab))\n",
    "    for sentence in tokenized:\n",
    "        for word in sentence: \n",
    "            if vocab[word] < minimum or vocab[word] > maximum:\n",
    "                del vocab[word]\n",
    "    print('%d vocabs after' % len(vocab))\n",
    "    \n",
    "    word2idx = {}\n",
    "    idx2word = {}\n",
    "    i = 0\n",
    "    while i < len(vocab):\n",
    "        for word in vocab.keys():\n",
    "            word2idx[word]=i\n",
    "            idx2word[i]=word\n",
    "            i += 1\n",
    "    return vocab, word2idx, idx2word\n",
    "\n",
    "def get_context(tokenized, vocab, windowsize): \n",
    "    #Function. \n",
    "    #Input: a list of tokenized strings \n",
    "    #Output: counter of word pairs, counting the number of times a word occurs in the context of another word \n",
    "    #Function only takes the words that are within the vocabulary into consideration \n",
    "    #Function builds three separate counters: word_pair, word1_count and word2_count \n",
    "    #Parameters: (1) tokenized: a list of tokenized strings \n",
    "    #Parameters: (2) vocab: vocabulary counter \n",
    "    #Parameters: (3) windowsize: context window size\n",
    "    #Return: (1) word_pair: counter where word_pair_count[(word1, word2)] is the count of word2 occurrence in word1 context window \n",
    "    #Return: (2) word1_count: counter where word1_count[w] is the number of times word1 occurred in the review \n",
    "    #Return: (3) word2_count: conter where word2_count[c] is the number of times word2 occurred in the review \n",
    "\n",
    "    word_pair_count = Counter()\n",
    "    word1_count = Counter()\n",
    "    word2_count = Counter()\n",
    "\n",
    "    for sentences in tokenized: \n",
    "        for i in np.arange(len(sentences)): \n",
    "            if sentences[i] in vocab.keys(): \n",
    "                target_index = i \n",
    "                windowstart = target_index - windowsize\n",
    "                windowend = target_index + windowsize \n",
    "                if windowstart < 0: \n",
    "                    windowstart = 0 \n",
    "                if windowend > len(sentences) -1: \n",
    "                    windowend = len(sentences)-1\n",
    "                for j in np.arange(windowstart, windowend+1):\n",
    "                    if j != target_index and sentences[j] in vocab.keys():\n",
    "                        word_pair_count[(sentences[i], sentences[j])] += 1\n",
    "                        word1_count[sentences[i]]+=1\n",
    "                        word2_count[sentences[j]]+=1\n",
    "    print(\"There are {} word-context pairs\".format(len(word_pair_count)))\n",
    "    return word_pair_count, word1_count, word2_count\n",
    "\n",
    "def get_PMI(word_pair_count, word1_count, word2_count, word2idx): \n",
    "    #Function, which returns the Positive Pointwise Mututal information of words in a vocabulary \n",
    "    #Input: word_pair_count: a Counter where word_pair_count[(w, c)] = count of c's occurences in w's context window\n",
    "    #Input: word1_count: counter where word1_count[w] = the number of times word1 occured in the documents\n",
    "    #Input: word2_count: counter where word2_count[w] = the number of times word2 occured in the documents\n",
    "    #Input: word2idx: a word-index loopup dictionary for word in a vocabulary \n",
    "    #Output: Positive Pointwise Mutual Information, a sparse matrix \n",
    "    new_data = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    total_occurences = sum(word_pair_count.values())\n",
    "    word1_occurences = sum(word1_count.values())\n",
    "    word2_occurences = sum(word2_count.values())\n",
    "    \n",
    "    for (w,c), n in word_pair_count.items():\n",
    "        p_word1 = word1_count[w]/word1_occurences\n",
    "        p_word2 = word2_count[c]/word2_occurences\n",
    "        PMI = np.math.log2((n/total_occurences)/(p_word1*p_word2))\n",
    "        new_data.append(max(0,PMI))\n",
    "        rows.append(word2idx[w])\n",
    "        cols.append(word2idx[c])\n",
    "\n",
    "    PPMI = csc_matrix((new_data, (rows, cols)))\n",
    "    return PPMI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(PPMI, rank):\n",
    "    #Function. Returns the left singular vectors as word embeddings. \n",
    "    #Function. Using truncated SVD. \n",
    "    #Input: PPMI: a sparce matrix of Positive Pointwise Information \n",
    "    #Input: rank: number of singular values and vectors to compute \n",
    "    #Return: u: left singular vectors from sparce SVD \n",
    "    #Returns: s: singular values from sparse SVD \n",
    "    u, s, vt = svds(PPMI, k=rank)\n",
    "    return u, s\n",
    "\n",
    "def cosine_distances(matrix, vector):\n",
    "    distances = []\n",
    "    for v in matrix:\n",
    "        distances.append(np.dot(vector,v)/(np.linalg.norm(vector)*np.linalg.norm(v)))\n",
    "    return  distances\n",
    "\n",
    "\n",
    "def nearest_neighbors(embeddings, word, k, word2idx, idx2word):\n",
    "    vector = embeddings[word2idx[word]]\n",
    "    distances = cosine_distances(embeddings, vector)\n",
    "    nearest_neighbors = []\n",
    "\n",
    "    idx = np.asarray(distances).argsort()[-(k+1):][::-1]\n",
    "    for i in idx:\n",
    "        if i != word2idx[word]:\n",
    "            nearest_neighbors.append(idx2word[i])\n",
    "    return nearest_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cooccurrance_matrix(df, column, maximum, minimum, windowsize): \n",
    "    listTokenisedReviews, numTokenisedReviews = get_reviews(df, column)\n",
    "    vocab, word2idx, idx2word = get_vocabulary(listTokenisedReviews, minimum, maximum)\n",
    "    word_pair_count, word1_count, word2_count = get_context(listTokenisedReviews, vocab, windowsize)\n",
    "    PPMI = get_PMI(word_pair_count, word1_count, word2_count, word2idx)\n",
    "    return word2idx, idx2word, vocab, PPMI\n",
    "\n",
    "def get_glossary(path, column, maximum, minimum, windowsize, num_neighbors): \n",
    "    df = read_data(path)\n",
    "    df = listToTuples(df)\n",
    "    word2idx, idx2word, vocab, PPMI = get_cooccurrance_matrix(df, column, maximum, minimum, windowsize)\n",
    "    rank = 20\n",
    "    embeddings, _ = get_embeddings(PPMI, rank)\n",
    "    embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True) \n",
    "    glossary = dict()\n",
    "    for words in vocab.keys(): \n",
    "        glossary[words] = nearest_neighbors(embeddings, words, num_neighbors, word2idx, idx2word)\n",
    "    glossary = pd.DataFrame(glossary.items(), columns=['word', 'similar'])\n",
    "    glossary = glossary.to_dict(orient='record')\n",
    "    return glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToTuples(df):\n",
    "    listToTuples = []\n",
    "    for i in df['sentiment_bigrams']: \n",
    "        lst = []\n",
    "        for j in i: \n",
    "            lst.append(tuple(j))\n",
    "        listToTuples.append(lst)\n",
    "    df['sentiment_bigrams'] = listToTuples\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13742 vocabs before\n",
      "900 vocabs after\n",
      "There are 63567 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 10100\n",
    "minimum = 1/100 * 10100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "music_unigram_glossary = get_glossary('data/processed/source_music.json', 'sentiment_unigrams', \n",
    "                                     maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45774 vocabs before\n",
      "1278 vocabs after\n",
      "There are 262930 word-context pairs\n",
      "33371 vocabs before\n",
      "1060 vocabs after\n",
      "There are 210311 word-context pairs\n",
      "28364 vocabs before\n",
      "835 vocabs after\n",
      "There are 144368 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 10100\n",
    "minimum = 1/200 * 10100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "book_unigram_glossary = get_glossary('data/processed/source_B.json', 'sentiment_unigrams', \n",
    "                                     maximum, minimum, windowsize, num_neighbors)\n",
    "electronics_unigram_glossary = get_glossary('data/processed/source_E.json', 'sentiment_unigrams', \n",
    "                                maximum, minimum, windowsize, num_neighbors)\n",
    "pet_unigram_glossary = get_glossary('data/processed/source_P.json', 'sentiment_unigrams', \n",
    "                                maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37323 vocabs before\n",
      "938 vocabs after\n",
      "There are 239543 word-context pairs\n",
      "46882 vocabs before\n",
      "1088 vocabs after\n",
      "There are 279832 word-context pairs\n",
      "49689 vocabs before\n",
      "1238 vocabs after\n",
      "There are 352390 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 14100\n",
    "minimum = 1/200 * 14100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "EP_unigram_glossary = get_glossary('data/processed/source_EP.json', 'sentiment_unigrams', \n",
    "                                 maximum, minimum, windowsize, num_neighbors)\n",
    "BP_unigram_glossary = get_glossary('data/processed/source_BP.json', 'sentiment_unigrams', \n",
    "                                 maximum, minimum, windowsize, num_neighbors)\n",
    "BE_unigram_glossary = get_glossary('data/processed/source_BE.json', 'sentiment_unigrams', \n",
    "                                 maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52819 vocabs before\n",
      "1197 vocabs after\n",
      "There are 408134 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 18100\n",
    "minimum = 1/200 * 18100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "multisource_unigram_glossary = get_glossary('data/processed/source_BEP.json', 'sentiment_unigrams', \n",
    "                                            maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48406 vocabs before\n",
      "400 vocabs after\n",
      "There are 1130 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 2100\n",
    "minimum = 1/500 * 2100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "music_bigram_glossary = get_glossary('data/processed/source_music.json', 'sentiment_bigrams', \n",
    "                                     maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307493 vocabs before\n",
      "1274 vocabs after\n",
      "There are 15869 word-context pairs\n",
      "234342 vocabs before\n",
      "930 vocabs after\n",
      "There are 8922 word-context pairs\n",
      "186085 vocabs before\n",
      "783 vocabs after\n",
      "There are 6820 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 10100\n",
    "minimum = 1/1000 * 10100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "books_bigram_glossary = get_glossary('data/processed/source_B.json', 'sentiment_bigrams', \n",
    "                                     maximum, minimum, windowsize, num_neighbors)\n",
    "electronics_bigram_glossary = get_glossary('data/processed/source_E.json', 'sentiment_bigrams', \n",
    "                                maximum, minimum, windowsize, num_neighbors)\n",
    "pet_bigram_glossary = get_glossary('data/processed/source_P.json', 'sentiment_bigrams', \n",
    "                                maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392047 vocabs before\n",
      "1103 vocabs after\n",
      "There are 15841 word-context pairs\n",
      "353230 vocabs before\n",
      "970 vocabs after\n",
      "There are 13001 word-context pairs\n",
      "353230 vocabs before\n",
      "970 vocabs after\n",
      "There are 13001 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 14100\n",
    "minimum = 1/1000 * 14100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "BE_bigram_glossary = get_glossary('data/processed/source_BE.json', 'sentiment_bigrams', \n",
    "                                            maximum, minimum, windowsize, num_neighbors)\n",
    "BP_bigram_glossary = get_glossary('data/processed/source_BP.json', 'sentiment_bigrams', \n",
    "                                            maximum, minimum, windowsize, num_neighbors)\n",
    "EP_bigram_glossary = get_glossary('data/processed/source_BP.json', 'sentiment_bigrams', \n",
    "                                            maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "455668 vocabs before\n",
      "492 vocabs after\n",
      "There are 9214 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 18100\n",
    "minimum = 1/600 * 18100\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "\n",
    "multisource_bigram_glossary = get_glossary('data/processed/source_BEP.json', 'sentiment_bigrams', \n",
    "                                            maximum, minimum, windowsize, num_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('data/glossary/', 'book_unigram_glossary.json', book_unigram_glossary)\n",
    "write_file('data/glossary/', 'electronics_unigram_glossary.json', electronics_unigram_glossary)\n",
    "write_file('data/glossary/', 'pet_unigram_glossary.json', pet_unigram_glossary)\n",
    "write_file('data/glossary/', 'EP_unigram_glossary.json', EP_unigram_glossary)\n",
    "write_file('data/glossary/', 'BE_unigram_glossary.json', BE_unigram_glossary)\n",
    "write_file('data/glossary/', 'BP_unigram_glossary.json', BP_unigram_glossary)\n",
    "write_file('data/glossary/', 'multisource_unigram_glossary.json', multisource_unigram_glossary)\n",
    "\n",
    "write_file('data/glossary/', 'books_bigram_glossary.json', books_bigram_glossary)\n",
    "write_file('data/glossary/', 'electronics_bigram_glossary.json', electronics_bigram_glossary)\n",
    "write_file('data/glossary/', 'pet_bigram_glossary.json', pet_bigram_glossary)\n",
    "write_file('data/glossary/', 'EP_bigram_glossary.json', EP_bigram_glossary)\n",
    "write_file('data/glossary/', 'BE_bigram_glossary.json', BE_bigram_glossary)\n",
    "write_file('data/glossary/', 'BP_bigram_glossary.json', BP_bigram_glossary)\n",
    "write_file('data/glossary/', 'multisource_bigram_glossary.json', multisource_bigram_glossary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_file('data/glossary/', 'music_unigram_glossary.json', music_unigram_glossary)\n",
    "write_file('data/glossary/', 'music_bigram_glossary.json', music_bigram_glossary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53573 vocabs before\n",
      "45 vocabs after\n",
      "There are 1981 word-context pairs\n",
      "53573 vocabs before\n",
      "59 vocabs after\n",
      "There are 3427 word-context pairs\n",
      "53573 vocabs before\n",
      "115 vocabs after\n",
      "There are 12460 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "2269 vocabs after\n",
      "There are 700386 word-context pairs\n",
      "53573 vocabs before\n",
      "5502 vocabs after\n",
      "There are 1169511 word-context pairs\n",
      "53573 vocabs before\n",
      "11060 vocabs after\n",
      "There are 1530815 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 18100\n",
    "minimum = [1000, 905, 603, 200, 45, 15,  6]\n",
    "windowsize = 3\n",
    "num_neighbors = 10\n",
    "for i in minimum: \n",
    "    multisource_unigram_glossary_test = get_glossary('data/processed/source_BEP.json', 'sentiment_unigrams', \n",
    "                                            maximum, i, windowsize, num_neighbors)\n",
    "    write_file('test/', 'multisource_unigram_glossary_test{}.json'.format(i), multisource_unigram_glossary_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n",
      "53573 vocabs before\n",
      "530 vocabs after\n",
      "There are 163700 word-context pairs\n"
     ]
    }
   ],
   "source": [
    "maximum = 1/10 * 18100\n",
    "minimum  = 200\n",
    "windowsize = 3\n",
    "neigh_size = [1,10, 20, 50, 100, 500, 1000]\n",
    "for i in neigh_size: \n",
    "    multisource_unigram_glossary_anothertest = get_glossary('data/processed/source_BEP.json', 'sentiment_unigrams', \n",
    "                                            maximum, minimum, windowsize, i)\n",
    "    write_file('test/', 'multisource_unigram_glossary_anothertest{}.json'.format(i), multisource_unigram_glossary_anothertest)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
